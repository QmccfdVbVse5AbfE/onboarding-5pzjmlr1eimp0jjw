{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attempting_to_scrape.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QmccfdVbVse5AbfE/onboarding-5pzjmlr1eimp0jjw/blob/master/attempting_to_scrape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXiaCF6xrQu_"
      },
      "source": [
        "# Install packages\n",
        "!pip install -U tmtoolkit\n",
        "!pip install matplotlib==3.1.1\n",
        "!pip install svgpath2mpl "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R66as8uN4Bu2",
        "outputId": "0c3389ad-e0da-45df-93be-9f8d7643fa8d"
      },
      "source": [
        "from collections import defaultdict\n",
        "import json\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import requests \n",
        "import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tools import eval_measures\n",
        "import copy\n",
        "\n",
        "# Text processing imports\n",
        "import spacy\n",
        "parser = spacy.load(\"en_core_web_sm\")\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from tmtoolkit.topicmod.evaluate import metric_coherence_gensim\n",
        "\n",
        "# Visualization imports\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from svgpath2mpl import parse_path\n",
        "\n",
        "# Statistical test imports\n",
        "import scipy\n",
        "import statsmodels\n",
        "from scipy.stats import friedmanchisquare\n",
        "from statsmodels.graphics.gofplots import qqplot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.2.5) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.7). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8YZuotmcQYO"
      },
      "source": [
        "# DATA DELIVERABLE (CHECKPOINT 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WogkkzkncU4C"
      },
      "source": [
        "## CONSTRUCT THE DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4V2KMGH5jeA",
        "outputId": "75fff1f6-c646-43b8-c428-562868df61c0"
      },
      "source": [
        "\n",
        "# Mount Google Drive locally\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# # Dataset from: https://www.whitehouse.gov/omb/historical-tables/\n",
        "# # NOTE: make sure the shared projet folder follows this path\n",
        "# with open('/content/drive/My Drive/CS1951A Final Project/hist05z2_fy22.csv', 'r', encoding='utf-8') as data:\n",
        "#   budget_df = pd.read_csv(data, header=2, index_col=\"Department or other unit\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFki3m8ylf9t",
        "outputId": "19771062-cc4a-4109-c8c7-d0094088336e"
      },
      "source": [
        "# Webscrape State of The Union Addresses\n",
        "# Dataset from: https://garden.org/plants/group/\n",
        "\n",
        "speech_table_url = \"https://garden.org/plants/group/\"\n",
        "def get_yearly_speeches():\n",
        "  headers = {'user-agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:77.0) Gecko/20190101 Firefox/77.0'}\n",
        "  # proxies = {'http': 'http://194.224.192.226','https': 'http://194.224.192.226',}\n",
        "  response = requests.get(speech_table_url, headers = headers)\n",
        "  print(response.text)\n",
        "\n",
        "  print(\"headers:\", response.request.headers)\n",
        "  print(\"response: \", response, type(response))\n",
        "  # print(\"response.text: \", response.text, type(response.text))\n",
        "  # print(\"response.content: \", response.content, type(response.text))\n",
        "  bs = BeautifulSoup(response.text, 'html.parser')\n",
        "  start = False\n",
        "  all_speeches_url = []\n",
        "  for link in bs.find_all('a'):\n",
        "    if link.has_attr('href'):\n",
        "      if(link.attrs['href'] == '/plants/group/daylilies/'):\n",
        "        start = True\n",
        "      if(start):\n",
        "        all_speeches_url.append('https://garden.org' + link.attrs['href'])\n",
        "      if(link.attrs['href'] == '/plants/group/zea/'):\n",
        "        start = False\n",
        "  return all_speeches_url\n",
        "all_generic_url = get_yearly_speeches()\n",
        "print(all_generic_url)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Excessive requests from your IP has resulted in an automatic block being added to our system. This is a protective mechanism to protect us against malicious bot traffic. If you are a regular visitor of the site, then please accept our apologies and know that we did not intend to block you. If you feel that your IP should not have been blocked, please contact us at admin@garden.org.<P>When you contact us, include your IP (which is 35.237.78.181) in your message.\n",
            "headers: {'user-agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:77.0) Gecko/20190101 Firefox/77.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}\n",
            "response:  <Response [200]> <class 'requests.models.Response'>\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHSsNvCvw-nd"
      },
      "source": [
        "# print(len(all_generic_url))\n",
        "\n",
        "def get_generic_id():\n",
        "  all_generic_links = []\n",
        "  for curr_link in all_generic_url:\n",
        "  # for curr_link in all_generic_url:\n",
        "  # curr_link = all_generic_url[0] #need to change when doing all\n",
        "    headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36'}\n",
        "    response = requests.get(curr_link, headers = headers)\n",
        "    print(\"headers:\", response.request.headers)\n",
        "    print(\"response: \", response, type(response))\n",
        "    bs = BeautifulSoup(response.text, 'html.parser')\n",
        "    sections = bs.find_all('div', {\"class\": \"panel-body\"})\n",
        "    if(len(sections) >= 3):\n",
        "      if(len(sections[2]) > 1):\n",
        "        specific_link = \"https://garden.org\" + sections[2].contents[1].attrs['href']\n",
        "        all_generic_links.append(specific_link)\n",
        "        print(specific_link)\n",
        "  \n",
        "  # print(sections[2].contents[1])\n",
        "  return all_generic_links\n",
        "  # link = sections.find('a')\n",
        "  # if link.has_attr('href'):\n",
        "  #   print(link.attrs['href'])\n",
        "test_plant = get_generic_id()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn1jJJJUENND"
      },
      "source": [
        "print(test_plant)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9PQ860wyPzL"
      },
      "source": [
        "print(test_plant)\n",
        "def get_20_popular():\n",
        "  all_plant_pages = []\n",
        "  # for curr_link in test_plant:\n",
        "  headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36'}\n",
        "  response = requests.get(test_plant[0] + 'popular/', headers = headers)\n",
        "  print(\"headers:\", response.request.headers)\n",
        "  print(\"response: \", response, type(response))\n",
        "  bs = BeautifulSoup(response.text, 'html.parser')\n",
        "  sections = bs.find_all('td')\n",
        "  print(sections)\n",
        "  for item in sections:\n",
        "    print(item.contents)\n",
        "    all_plant_pages.append(item.attrs['href'])\n",
        "  return all_plant_pages\n",
        "# popular = get_20_popular()\n",
        "# print(popular)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcK4mrJnEsri"
      },
      "source": [
        "min_id = 2\n",
        "max_id = 783799\n",
        "max_id = 10\n",
        "all_valid_ids = range(min_id, max_id)\n",
        "def get_all_subtypes():\n",
        "  all_plant_pages = []\n",
        "  base_url = \"https://garden.org/plants/view/794278/Chinese-Wisteria-Wisteria-sinensis-Variegata/\"\n",
        "  for i in all_valid_ids:\n",
        "    whole_url = base_url + '/' + str(i)\n",
        "    headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36'}\n",
        "    response = requests.get(whole_url, headers = headers)\n",
        "    print(\"headers:\", response.request.headers)\n",
        "    print(\"response: \", response, type(response))\n",
        "    # print(\"response.text: \", response.text, type(response.text))\n",
        "    all_plant_pages.append(response.text)\n",
        "\n",
        "  return all_plant_pages\n",
        "\n",
        "all_plants = get_all_subtypes()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxUzQWR5rwI3"
      },
      "source": [
        "# print(all_plants[0])\n",
        "\n",
        "bs = BeautifulSoup(all_plants[0], 'html.parser')\n",
        "for link in bs.find_all('tbody'):\n",
        "  print(link)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJhqLSszbcPZ"
      },
      "source": [
        "for speech in all_speeches_url:\n",
        "  # print(speech['president'])\n",
        "  if(speech['president'] == 'gerald r. ford'):\n",
        "    break\n",
        "  response = requests.get(speech['url'])\n",
        "  html_dump = BeautifulSoup(response.text, 'html.parser')\n",
        "  # cols = html_tables.find(\"tbody\")\n",
        "  stocks = html_dump.find_all(\"section\")\n",
        "  full_text_together = \"\"\n",
        "  for i in stocks:\n",
        "    full_text_broken = i.find_all(\"p\")\n",
        "    # print(\"text\", full_text_broken)\n",
        "    for j in full_text_broken:\n",
        "      # print(\"text\", full_text)\n",
        "      full_text_together += j.text + \" \"\n",
        "      speech['speech'] = (full_text_together)\n",
        "# print(all_speeches_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YKKCSimc8V1"
      },
      "source": [
        "# Convert to pandas DF\n",
        "speeches_df = pd.DataFrame(all_speeches_url)\n",
        "speeches_df['president'][0] = 'joe biden'\n",
        "speeches_df.head(48)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQkdKhVHxszl"
      },
      "source": [
        "Load in health dictionary dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3azG7N4o-7e"
      },
      "source": [
        "with open('/content/drive/My Drive/CS1951A Final Project/health_dictionary.txt', 'r', encoding='utf-8') as data:\n",
        "  items =  data.readlines()\n",
        "  health_dic = [x.strip().lower() for x in items] \n",
        "print(health_dic)\n",
        "\n",
        "health_dic.append([\"insurance\", \"healthcare\"])\n",
        "print(len(health_dic))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FNjyyJGcq8L"
      },
      "source": [
        "# CLEAN AND PROCESS DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acgmNk0G3aLJ"
      },
      "source": [
        "# Filter budget_df and transform strings to floats\n",
        "health_budget_df = budget_df.loc[['Department of Health and Human Services', 'Total budget authority']]\n",
        "health_budget_df.loc['Department of Health and Human Services'] = health_budget_df.loc['Department of Health and Human Services'].apply(lambda x: float(x.split()[0].replace(',', '')))\n",
        "health_budget_df.loc['Total budget authority'] = health_budget_df.loc['Total budget authority'].apply(lambda x: float(x.split()[0].replace(',', '')))\n",
        "\n",
        "# Transform budget_df into percentage (dept of health & human services / total budget authority)\n",
        "percent_health_budget_series = pd.Series(health_budget_df.loc['Department of Health and Human Services'].divide(health_budget_df.loc['Total budget authority']), name='Percent health budget spend')\n",
        "transformed_health_budget_df = health_budget_df.append(percent_health_budget_series)\n",
        "\n",
        "# Drop columns past 2020 (speeches dataset until 2020) and TQ\n",
        "transformed_health_budget_df = transformed_health_budget_df.drop(labels=['TQ', '2021 estimate', '2022 estimate', '2023 estimate', '2024 estimate', '2025 estimate', '2026 estimate'], axis=1)\n",
        "\n",
        "transformed_health_budget_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPu2jwEuYHgU"
      },
      "source": [
        "print(f\"Raw budget df shape: {budget_df.shape}\")\n",
        "print(f\"Health budget df shape: {transformed_health_budget_df.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdZ55ayReYrX"
      },
      "source": [
        "print(f\"Raw number of speeches: {speeches_df.shape}\")\n",
        "print(f\"Raw number of Presidents: {speeches_df.groupby(by=['president'] , axis=0).count()['speech']}\")\n",
        "\n",
        "print(speeches_df[speeches_df['president'] == 'gerald r. ford'])\n",
        "\n",
        "# Drop N/A rows in speeches data\n",
        "speeches_df_filtered = speeches_df.dropna(axis=0, subset=['speech'])\n",
        "# Removes \"about search\"\n",
        "speeches_df_filtered[\"speech\"] = speeches_df_filtered[\"speech\"].apply(lambda x: x[14:])\n",
        "\n",
        "# Filter out Joe Biden from dataframe (see reasoning in write-up)\n",
        "speeches_df_filtered_with_biden = speeches_df_filtered.copy(deep=True)\n",
        "speeches_df_filtered = speeches_df_filtered[speeches_df_filtered['president'] != 'joe biden']\n",
        "\n",
        "print(f\"Transformed number of speeches: {speeches_df_filtered.shape}\")\n",
        "print(f\"Transformed number of Presidents: {speeches_df_filtered.groupby(by=['president'] , axis=0).count()['speech']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tari9buEecfZ"
      },
      "source": [
        "# Include partisanship for each president\n",
        "conditions = [\n",
        "              speeches_df_filtered['president'] == 'donald j. trump',\n",
        "              speeches_df_filtered['president'] == 'barack obama',\n",
        "              speeches_df_filtered['president'] == 'george w. bush',\n",
        "              speeches_df_filtered['president'] == 'william j. clinton',\n",
        "              speeches_df_filtered['president'] == 'george bush',\n",
        "              speeches_df_filtered['president'] == 'ronald reagan',\n",
        "              speeches_df_filtered['president'] == 'jimmy carter',\n",
        "]\n",
        "\n",
        "is_dem = ['False', 'True', 'False', 'True', 'False', 'False', 'True']\n",
        "speeches_df_filtered['is_democrat'] = np.select(conditions, is_dem)\n",
        "\n",
        "speeches_df_filtered.head()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}